{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tatoeba: PyTorch Embeddings\n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notbook, we will build embedding objects and embedding bags \n",
    "This is a preliminary step to understand language detection and CLD3, https://github.com/google/cld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe3a8ad8070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(4321)\n",
    "torch.manual_seed(4321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use the class `Embedding(num_embeddings, embedding_dim, ...)` and we create 8 dense vectors (embeddings) in a vector space of dimension 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4716, -0.3436, -1.1742,  0.1221,  1.3231],\n",
       "        [-0.6415,  0.8538, -1.8969,  0.2142,  1.1937],\n",
       "        [-0.8704,  0.2439, -0.0453,  1.4172, -0.0614],\n",
       "        [ 1.5471,  1.4126,  0.0268,  0.5757, -0.8794],\n",
       "        [-0.0493,  0.0144, -0.3218, -0.1144, -0.6089],\n",
       "        [-0.1303,  0.1426,  1.6467,  0.8824, -0.8752],\n",
       "        [-0.4935,  0.4820, -0.6308, -0.1754,  0.3182],\n",
       "        [ 1.7125, -1.5122,  0.5076,  0.1487,  0.4369]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding layer acts as a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5471,  1.4126,  0.0268,  0.5757, -0.8794],\n",
       "        [-0.8704,  0.2439, -0.0453,  1.4172, -0.0614]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor([3, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the input has a variable length, we have to align vectors up to a maximal length. We need then a padding symbol for the sequences less than this maximal length. We tell Torch by assigning the padding symbol an index usually 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(8, 5, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2780, -0.2379,  0.2242, -0.0326, -0.9581],\n",
       "        [-1.5594, -0.2332,  0.6088, -0.5317, -1.3245],\n",
       "        [ 2.0561, -0.8149, -0.7872, -2.3632,  1.0788],\n",
       "        [ 0.8716,  0.2437, -1.5932,  0.6259, -0.4487],\n",
       "        [-0.2243,  0.1594,  0.7884, -0.1794, -0.3420],\n",
       "        [-1.0898, -0.2070,  0.1724,  0.1432,  0.8795],\n",
       "        [ 0.0064, -0.6666,  0.9459,  1.7336,  0.2753]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.0561, -0.8149, -0.7872, -2.3632,  1.0788],\n",
       "        [-1.5594, -0.2332,  0.6088, -0.5317, -1.3245]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor([0, 3, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Bags\n",
    "Embedding bags deal with embedding sequences of variable length when the embeddings are summed. In CLD3, we have a weighted sum of a variable number of embeddings. See https://github.com/google/cld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_bag = nn.EmbeddingBag(8, 5, mode='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.6391, -0.1375,  0.0677, -0.0138, -0.4869],\n",
       "        [-0.7907,  2.7215,  0.1517, -1.2534,  0.0896],\n",
       "        [ 1.2457,  0.8432, -0.0916,  1.0716, -1.1500],\n",
       "        [-1.2355,  2.1430, -0.3934,  0.0314, -0.6845],\n",
       "        [-3.5251, -2.2849, -0.7089,  1.5505, -0.4069],\n",
       "        [-0.0530,  0.5492,  1.4681, -0.4333,  0.0309],\n",
       "        [-0.5371, -1.9131,  0.6755,  1.3665,  0.7476],\n",
       "        [-0.8257, -0.0985,  0.3710, -1.2320,  0.1728]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an `EmbeddingBag` object needs the bags of indices it will sum as its first parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4549,  3.5647,  0.0601, -0.1818, -1.0604],\n",
       "        [-4.7606, -0.1419, -1.1023,  1.5819, -1.0914]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4549,  3.5647,  0.0601, -0.1818, -1.0604], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag.weight[1] + embedding_bag.weight[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we may have a 1-D input and the the bag indices as second parameter: `offsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4549,  3.5647,  0.0601, -0.1818, -1.0604],\n",
       "        [-4.7606, -0.1419, -1.1023,  1.5819, -1.0914]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([1, 2, 3, 4]), offsets=torch.tensor([0, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute a weighted sum using the `per_sample_weights` parameter. The shape must be the same as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2275,  1.7823,  0.0301, -0.0909, -0.5302],\n",
       "        [-3.0672, -1.3993, -0.6458,  1.2467, -0.4624]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1, 2], [3, 4]]), per_sample_weights=torch.tensor([[0.5, 0.5], [0.2, 0.8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2275,  1.7823,  0.0301, -0.0909, -0.5302], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * embedding_bag.weight[1] + 0.5 * embedding_bag.weight[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0672, -1.3993, -0.6458,  1.2467, -0.4624], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2 * embedding_bag.weight[3] + 0.8 * embedding_bag.weight[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2275,  1.7823,  0.0301, -0.0909, -0.5302],\n",
       "        [-3.0672, -1.3993, -0.6458,  1.2467, -0.4624]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([1, 2, 3, 4]), \n",
    "              offsets=torch.tensor([0, 2]), \n",
    "              per_sample_weights=torch.tensor([0.5, 0.5, 0.2, 0.8]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
