{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tatoeba: PyTorch Embeddings\n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notbook, we will build embedding objects and embedding bags \n",
    "This is a preliminary step to understand language detection and CLD3, https://github.com/google/cld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x131ad64d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use the class `Embedding(num_embeddings, embedding_dim, ...)` and we create 8 dense vectors (embeddings) in a vector space of dimension 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4562, -0.9391,  1.9130, -0.7439, -0.4459],\n",
       "        [-1.2780, -0.2379,  0.2242, -0.0326, -0.9581],\n",
       "        [-1.5594, -0.2332,  0.6088, -0.5317, -1.3245],\n",
       "        [ 2.0561, -0.8149, -0.7872, -2.3632,  1.0788],\n",
       "        [ 0.8716,  0.2437, -1.5932,  0.6259, -0.4487],\n",
       "        [-0.2243,  0.1594,  0.7884, -0.1794, -0.3420],\n",
       "        [-1.0898, -0.2070,  0.1724,  0.1432,  0.8795],\n",
       "        [ 0.0064, -0.6666,  0.9459,  1.7336,  0.2753]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding layer acts as a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0561, -0.8149, -0.7872, -2.3632,  1.0788],\n",
       "        [-1.5594, -0.2332,  0.6088, -0.5317, -1.3245]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor([3, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the input has a variable length, we have to align vectors up to a maximal length. We need then a padding symbol for the sequences less than this maximal length. We tell Torch by assigning the padding symbol an index usually 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(8, 5, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8, 5, padding_idx=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2780, -0.2379,  0.2242, -0.0326, -0.9581],\n",
       "        [-1.5594, -0.2332,  0.6088, -0.5317, -1.3245],\n",
       "        [ 2.0561, -0.8149, -0.7872, -2.3632,  1.0788],\n",
       "        [ 0.8716,  0.2437, -1.5932,  0.6259, -0.4487],\n",
       "        [-0.2243,  0.1594,  0.7884, -0.1794, -0.3420],\n",
       "        [-1.0898, -0.2070,  0.1724,  0.1432,  0.8795],\n",
       "        [ 0.0064, -0.6666,  0.9459,  1.7336,  0.2753]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2355,  2.1430, -0.3934,  0.0314, -0.6845],\n",
       "        [ 1.2457,  0.8432, -0.0916,  1.0716, -1.1500]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor([0, 3, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Bags\n",
    "Embedding bags deal with embedding sequences of variable length when the embeddings are summed. In CLD3, we have a weighted sum of a variable number of embeddings. See https://github.com/google/cld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_bag = nn.EmbeddingBag(8, 5, mode='sum')  # Default mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.7122,  0.3235, -0.6941, -0.1128, -0.3620],\n",
       "        [-0.0215, -0.3685, -0.2244,  2.4480, -1.4205],\n",
       "        [-1.0848,  0.3204, -1.3139,  0.8290, -0.2611],\n",
       "        [ 0.3025, -1.0016, -0.2019,  0.0992, -1.1972],\n",
       "        [ 1.4835, -0.5860,  0.7933, -0.7355, -0.9182],\n",
       "        [-1.0781,  0.0139,  0.6118,  1.1398,  0.1750],\n",
       "        [ 1.0420,  0.1025,  0.6203, -0.2963,  0.2832],\n",
       "        [ 0.8905, -0.9703,  1.7193, -0.1643,  1.8135]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an `EmbeddingBag` object needs the bags of indices it will sum as its first parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1063, -0.0481, -1.5382,  3.2771, -1.6816],\n",
       "        [ 1.7861, -1.5875,  0.5914, -0.6363, -2.1154]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1063, -0.0481, -1.5382,  3.2771, -1.6816]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1]])) + embedding_bag(torch.tensor([[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7861, -1.5875,  0.5914, -0.6363, -2.1154], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag.weight[3] + embedding_bag.weight[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we may have a 1-D input and the the bag indices as second parameter: `offsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1063, -0.0481, -1.5382,  3.2771, -1.6816],\n",
       "        [ 1.7861, -1.5875,  0.5914, -0.6363, -2.1154]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([1, 2, 3, 4]), offsets=torch.tensor([0, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute a weighted sum using the `per_sample_weights` parameter. The shape must be the same as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5532, -0.0241, -0.7691,  1.6385, -0.8408],\n",
       "        [ 1.2473, -0.6691,  0.5942, -0.5686, -0.9740]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([[1, 2], [3, 4]]), per_sample_weights=torch.tensor(\n",
    "    [[0.5, 0.5], [0.2, 0.8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5532, -0.0241, -0.7691,  1.6385, -0.8408], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * embedding_bag.weight[1] + 0.5 * embedding_bag.weight[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2473, -0.6691,  0.5942, -0.5686, -0.9740], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2 * embedding_bag.weight[3] + 0.8 * embedding_bag.weight[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5532, -0.0241, -0.7691,  1.6385, -0.8408],\n",
       "        [ 1.2473, -0.6691,  0.5942, -0.5686, -0.9740]],\n",
       "       grad_fn=<EmbeddingBagBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_bag(torch.tensor([1, 2, 3, 4]),\n",
    "              offsets=torch.tensor([0, 2]),\n",
    "              per_sample_weights=torch.tensor([0.5, 0.5, 0.2, 0.8]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
